{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f256944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install x-unet\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cbbefa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "def set_seed(seed: int = 0):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cea752ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8d388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_path = 'data'\n",
    "png_folder = os.path.join(data_path, 'images')\n",
    "\n",
    "checkpoints_path = 'checkpoints'\n",
    "os.makedirs(checkpoints_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "802e4a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "class HighFrequencyDataset(Dataset):\n",
    "    def __init__(self, images_path):\n",
    "        self.images_path = images_path\n",
    "        self.images = self.read_images()\n",
    "\n",
    "    def read_images(self) -> torch.Tensor:\n",
    "        image_file_paths = [f for f in os.listdir(self.images_path) if f.lower().endswith('.png')]\n",
    "        images = [Image.open(os.path.join(self.images_path, img_path)) for img_path in image_file_paths]\n",
    "        return self.transform_images(images)\n",
    "\n",
    "    def transform_images(self, images: list[Image]) -> torch.Tensor:\n",
    "        transform = T.Compose([\n",
    "            T.Resize((256, 256)),\n",
    "            T.ToTensor(),\n",
    "        ])\n",
    "        return torch.stack([transform(image) for image in images])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx) -> torch.Tensor:\n",
    "        return self.images[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b5864ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = HighFrequencyDataset(png_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2682e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset, val_dataset = train_test_split(dataset, test_size=0.2, random_state=0)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "415966cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from x_unet import XUnet\n",
    "\n",
    "# Create an instance of the XUnet model\n",
    "unet = XUnet(\n",
    "    dim = 64,\n",
    "    channels = 1,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    nested_unet_depths = (7, 4, 2, 1),     # nested unet depths, from unet-squared paper\n",
    "    consolidate_upsample_fmaps = True,     # whether to consolidate outputs from all upsample blocks, used in unet-squared paper\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e09275b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "unet = unet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8bf07e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gaussian_noise(x: torch.Tensor, std: float):\n",
    "    # x in [-1, 1]; scale to [-1,1] noise as well\n",
    "    noise = torch.randn_like(x) * std\n",
    "    return (x + noise).clamp(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2fe8ca7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_394333/1417274608.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=MIXED_PRECISION and device.type == \"cuda\")\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "BETAS = (0.9, 0.99)\n",
    "WEIGHT_DECAY = 1e-4\n",
    "MIXED_PRECISION = True\n",
    "\n",
    "# Define optimizer\n",
    "opt = torch.optim.AdamW(unet.parameters(), lr=LEARNING_RATE, betas=BETAS, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Define scaler (so we can use mixed precision), otherwise, we'll experience vanishing gradients\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=MIXED_PRECISION and device.type == \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fbbc321b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46/46 [00:20<00:00,  2.29it/s, loss=0.0008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train 0.0011 | val 0.0011 | best 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:  39%|███████████████████████████████████████████▍                                                                   | 18/46 [00:08<00:13,  2.15it/s, loss=0.0008]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m     loss = F.mse_loss(pred, x)     \u001b[38;5;66;03m# DAE loss\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m scaler.is_enabled():\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     scaler.scale(loss).backward()\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m CLIP_GRAD \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     34\u001b[39m         scaler.unscale_(opt)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/users/kolodny/gyanai/miniconda3/lib/python3.13/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m torch.autograd.backward(\n\u001b[32m    649\u001b[39m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs=inputs\n\u001b[32m    650\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/users/kolodny/gyanai/miniconda3/lib/python3.13/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m _engine_run_backward(\n\u001b[32m    354\u001b[39m     tensors,\n\u001b[32m    355\u001b[39m     grad_tensors_,\n\u001b[32m    356\u001b[39m     retain_graph,\n\u001b[32m    357\u001b[39m     create_graph,\n\u001b[32m    358\u001b[39m     inputs,\n\u001b[32m    359\u001b[39m     allow_unreachable=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    360\u001b[39m     accumulate_grad=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    361\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/users/kolodny/gyanai/miniconda3/lib/python3.13/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable._execution_engine.run_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    825\u001b[39m         t_outputs, *args, **kwargs\n\u001b[32m    826\u001b[39m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "NOISE_STD = 0.1\n",
    "CLIP_GRAD = 1\n",
    "SAMPLE_EVERY = 1000\n",
    "EPOCHS = 10\n",
    "MODEL_OUT_DIR = os.path.join(checkpoints_path, 'vanilla')\n",
    "\n",
    "os.makedirs(MODEL_OUT_DIR, exist_ok=True)\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    unet.train()\n",
    "    pbar = tqdm(iter(train_loader), desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
    "    running = 0.0\n",
    "\n",
    "    for x in pbar:\n",
    "        x = x.to(device)          # clean target in [-1,1]\n",
    "        x_noisy = add_gaussian_noise(x, NOISE_STD)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.autocast(device_type=device.type, dtype=torch.float16, enabled=scaler.is_enabled()):\n",
    "            pred = unet(x_noisy)          # predict clean image directly\n",
    "            loss = F.mse_loss(pred, x)     # DAE loss\n",
    "\n",
    "        if scaler.is_enabled():\n",
    "            scaler.scale(loss).backward()\n",
    "            if CLIP_GRAD is not None:\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(unet.parameters(), CLIP_GRAD)\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            if CLIP_GRAD is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(unet.parameters(), CLIP_GRAD)\n",
    "            opt.step()\n",
    "\n",
    "        running += loss.item()\n",
    "        global_step += 1\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        # sample preview\n",
    "        # if global_step % args.sample_every == 0:\n",
    "        #     with torch.no_grad():\n",
    "        #         grid = denoise_grid(model, x_noisy[:min(16, x_noisy.size(0))])\n",
    "        #     vutils.save_image(grid, out_dir / \"samples\" / f\"train_step{global_step:07d}.png\")\n",
    "\n",
    "    train_loss = running / len(train_loader)\n",
    "\n",
    "    # validation\n",
    "    unet.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xv in iter(val_loader):\n",
    "            xv = xv.to(device)\n",
    "            xv_noisy = add_gaussian_noise(xv, NOISE_STD)\n",
    "            pv = unet(xv_noisy)\n",
    "            val_loss += F.mse_loss(pv, xv, reduction='mean').item()\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    # track best\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        torch.save(unet.state_dict(), os.path.join(MODEL_OUT_DIR, \"best_model.pt\"))\n",
    "\n",
    "    print(f\"Epoch {epoch} | train {train_loss:.4f} | val {val_loss:.4f} | best {best_val:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
